{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpAZpjCGwqI3",
        "outputId": "f8fdc4c7-7818-4f55-b66f-5e307ba23dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "siRXliViwo6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZLRh1R4uZoJ",
        "outputId": "a6bcc6a4-863d-4f0e-e3ab-0dd45cd457bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q:\n",
            " tensor([[0.3238, 0.9859, 0.5374],\n",
            "        [0.0121, 0.1617, 0.0553],\n",
            "        [0.5467, 0.5247, 0.3065],\n",
            "        [0.3482, 0.1910, 0.0825],\n",
            "        [0.6001, 0.9960, 0.6995],\n",
            "        [0.6826, 0.7340, 0.9003]])\n",
            "K:\n",
            " tensor([[0.2702, 0.9027, 0.5765],\n",
            "        [0.6443, 0.8890, 0.9124],\n",
            "        [0.6316, 0.6336, 0.1246],\n",
            "        [0.3250, 0.4294, 0.2149],\n",
            "        [0.0456, 0.6882, 0.1571],\n",
            "        [0.6904, 0.4364, 0.8812]])\n",
            "V:\n",
            " tensor([[0.4470, 0.0603, 0.8035],\n",
            "        [0.3097, 0.8499, 0.5492],\n",
            "        [0.3506, 0.7157, 0.2504],\n",
            "        [0.3466, 0.3883, 0.1323],\n",
            "        [0.9092, 0.4529, 0.8132],\n",
            "        [0.9157, 0.8571, 0.5813]])\n"
          ]
        }
      ],
      "source": [
        "# Dữ liệu mẫu\n",
        "test_sentences = \"I love ML and DL technology\"\n",
        "tokens = test_sentences.split()\n",
        "n = len(tokens)\n",
        "d = 3\n",
        "\n",
        "# Tạo Q, K, V ngẫu nhiên (thay bằng embedding thực tế trong ứng dụng)\n",
        "Q = torch.rand(n, d)\n",
        "K = torch.rand(n, d)\n",
        "V = torch.rand(n, d)\n",
        "\n",
        "print(\"Q:\\n\", Q)\n",
        "print(\"K:\\n\", K)\n",
        "print(\"V:\\n\", V)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def full_attention(Q, K, V):\n",
        "    d_k = Q.size(-1)\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "    attn_weights = torch.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attn_weights, V)\n",
        "    return output\n",
        "\n",
        "result_full = full_attention(Q, K, V)\n",
        "print(\"Full Attention Output:\\n\", result_full)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHWwh5UWw0bX",
        "outputId": "5ef8d82c-c0a6-41fe-e810-7ada3d9ecb83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Attention Output:\n",
            " tensor([[0.5340, 0.5674, 0.5406],\n",
            "        [0.5450, 0.5542, 0.5249],\n",
            "        [0.5374, 0.5730, 0.5273],\n",
            "        [0.5418, 0.5647, 0.5211],\n",
            "        [0.5321, 0.5800, 0.5396],\n",
            "        [0.5360, 0.5901, 0.5380]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sliding_window_attention(Q, K, V, w=2):\n",
        "    n, d_k = Q.size() # => [số token, Q_dim]\n",
        "    output = torch.zeros_like(Q)\n",
        "    for i in range(n):\n",
        "        # Xác định các token xung quanh token i trong vùng cửa sổ (window)\n",
        "        # kích thước cửa sổ w = 2 (tức +- 1 token)\n",
        "        start = max(0, i - w//2)\n",
        "        end = min(n, i + w//2 + 1)\n",
        "        # Xác định vị trí của Q\n",
        "        Q_i = Q[i:i+1]\n",
        "        K_w = K[start:end]\n",
        "        V_w = V[start:end]\n",
        "\n",
        "        scores = torch.matmul(Q_i, K_w.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "        output[i] = torch.matmul(attn_weights, V_w).squeeze(0)\n",
        "    return output\n",
        "\n",
        "result_sliding = sliding_window_attention(Q, K, V, w=2)\n",
        "print(\"Sliding Window Attention Output:\\n\", result_sliding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PIum8yxw2SN",
        "outputId": "20bc72f7-bda0-4d21-bdd7-326ac54517d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sliding Window Attention Output:\n",
            " tensor([[0.3727, 0.4879, 0.6658],\n",
            "        [0.3691, 0.5411, 0.5379],\n",
            "        [0.3331, 0.6783, 0.3372],\n",
            "        [0.5268, 0.5250, 0.3905],\n",
            "        [0.7522, 0.6058, 0.5215],\n",
            "        [0.9133, 0.7067, 0.6676]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dilated_sliding_window_attention(Q, K, V, w=2, d=2):\n",
        "    n, d_k = Q.size() # => [số token, Q_dim]\n",
        "    output = torch.zeros_like(Q)\n",
        "    for i in range(n):\n",
        "        indices = []\n",
        "        # Thu thập vị trí của Key mà Query có thể liên hệ đến với bước nhảy d.\n",
        "        # Ví dụ: w = 2 (+-1) => cho i = 3: j = 3 - 2 = 1 (bên trái) j = 3 + 2 = 5(bên phải)\n",
        "        for k in range(-w//2, w//2 + 1):\n",
        "            j = i + k * d\n",
        "            if 0 <= j < n:\n",
        "                indices.append(j)\n",
        "        Q_i = Q[i:i+1]\n",
        "        K_w = K[indices]\n",
        "        V_w = V[indices]\n",
        "        scores = torch.matmul(Q_i, K_w.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "        output[i] = torch.matmul(attn_weights, V_w).squeeze(0)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "result_dilated = dilated_sliding_window_attention(Q, K, V, w=2, d=2)\n",
        "print(\"Dilated Sliding Window Attention Output:\\n\", result_dilated)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0sxPj-YQw3_s",
        "outputId": "1f7a808a-57f9-4f48-8be9-83d79933327b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dilated Sliding Window Attention Output:\n",
            " tensor([[0.4042, 0.3511, 0.5580],\n",
            "        [0.3276, 0.6269, 0.3478],\n",
            "        [0.5492, 0.4015, 0.6155],\n",
            "        [0.5270, 0.7113, 0.4327],\n",
            "        [0.6078, 0.5947, 0.5095],\n",
            "        [0.7000, 0.6794, 0.4111]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def global_sliding_attention(Q, K, V, w=2, global_indices=[0]):\n",
        "    n, d_k = Q.size()\n",
        "    output = torch.zeros_like(Q)\n",
        "\n",
        "    for i in range(n):\n",
        "        # Nếu i là global => tính full attention\n",
        "        if i in global_indices:\n",
        "            scores = torch.matmul(Q[i:i+1], K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "            attn_weights = torch.softmax(scores, dim=-1)\n",
        "            output[i] = torch.matmul(attn_weights, V).squeeze(0)\n",
        "        else:\n",
        "            start = max(0, i - w//2)\n",
        "            end = min(n, i + w//2 + 1)\n",
        "            indices = list(range(start, end)) + global_indices\n",
        "            indices = sorted(set([j for j in indices if 0 <= j < n]) )\n",
        "            Q_i = Q[i:i+1]\n",
        "            K_w = K[indices]\n",
        "            scores = torch.matmul(Q_i, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "            attn_weights = torch.softmax(scores, dim=-1)\n",
        "            output[i] = torch.matmul(attn_weights, V).squeeze(0)\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "result_global = global_sliding_attention(Q, K, V, w=2, global_indices=[0])\n",
        "print(\"Global + Sliding Window Attention Output:\\n\", result_global)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FCI7BQ3w5pd",
        "outputId": "2c59d9f3-5fb9-4a14-9893-98c8908308be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global + Sliding Window Attention Output:\n",
            " tensor([[0.5340, 0.5674, 0.5406],\n",
            "        [0.5450, 0.5542, 0.5249],\n",
            "        [0.5374, 0.5730, 0.5273],\n",
            "        [0.5418, 0.5647, 0.5211],\n",
            "        [0.5321, 0.5800, 0.5396],\n",
            "        [0.5360, 0.5901, 0.5380]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y65CsEvUxdj9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}