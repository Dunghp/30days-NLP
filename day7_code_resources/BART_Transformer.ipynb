{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RUgn56n_GTR"
      },
      "source": [
        "# **Mask Filling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKmFxFU69ctD",
        "outputId": "a12259ab-db3c-4c97-b23c-e7e8dd867d44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.9474, 0.0067, 0.0044], grad_fn=<TopkBackward0>) tensor([  5, 392, 273])\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
        "from huggingface_hub import login\n",
        "\n",
        "# login(\"huggingface API key here\")\n",
        "\n",
        "# Tải bộ tokenizer cho mô hình BART (cơ bản) từ Hugging Face hub\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "# Tải mô hình BART đã được pre-trained\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "TXT = \"When is you birthday, Trang? - It's on <mask> fifteenth of May\"\n",
        "input_ids = tokenizer(TXT, return_tensors=\"pt\")[\"input_ids\"]\n",
        "logits = model(input_ids).logits\n",
        "\n",
        "# Tìm từ match\n",
        "masked_index = (input_ids[0] == tokenizer.mask_token_id).nonzero().item()\n",
        "# Mỗi từ match => có phần trăm (trọng số) đi kèm\n",
        "probs = logits[0, masked_index].softmax(dim=0)\n",
        "values, predictions = probs.topk(3)\n",
        "\n",
        "# print(values, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GG4QxLrOHmsG",
        "outputId": "1f9e8f12-f56a-40c9-cb61-25d6aa73a015"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the', 'May', 'Friday']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.decode(predictions).split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GHkXA4IJClN"
      },
      "source": [
        "# **Text Summarization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KyPBhHyCVj5",
        "outputId": "3bcdf395-cbe2-4bed-9761-8f20ddb65876"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['</s><s>PG&E scheduled the blackouts in response to forecasts for high winds amid dry conditions</s>']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
        "\n",
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "ARTICLE_TO_SUMMARIZE = (\n",
        "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
        "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
        "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
        ")\n",
        "\n",
        "inputs = tokenizer(ARTICLE_TO_SUMMARIZE, max_length=1024, return_tensors=\"pt\")\n",
        "\n",
        "# Kết quả trả về là một đoạn tóm tắt của văn bản nhưng dưới dạng id\n",
        "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)\n",
        "tokenizer.batch_decode(summary_ids, skip_special_token=True, clean_up_tokenization_spaces=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHeGnrR0LPzE"
      },
      "source": [
        "# **Text Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "aa1OdwDECjT2",
        "outputId": "26d37352-f699-47c9-c99e-a99e5e7d0041"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'NEGATIVE'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, BartForSequenceClassification\n",
        "\n",
        "# valhalla/bart-large-sst2\n",
        "model = BartForSequenceClassification.from_pretrained(\"valhalla/bart-large-sst2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"valhalla/bart-large-sst2\")\n",
        "\n",
        "inputs = tokenizer(\"I hate programming and doing AI projects\", return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = model(**inputs).logits\n",
        "\n",
        "predicted_class_id = logits.argmax().item()\n",
        "model.config.id2label[predicted_class_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KnRqtP8QWce"
      },
      "source": [
        "# **Question Answering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "vHgbzw7WOu3i",
        "outputId": "37b5c946-1905-4464-c1db-fa59cfeee7c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n",
            "You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' a business magnate, industrial designer, and engineer'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, BartForQuestionAnswering\n",
        "\n",
        "# valhalla/bart-large-finetuned-squadv1\n",
        "model = BartForQuestionAnswering.from_pretrained(\"valhalla/bart-large-finetuned-squadv1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"valhalla/bart-large-finetuned-squadv1\")\n",
        "\n",
        "question = \"Who is Elon Musk?\"\n",
        "text = \"Elon Musk is a business magnate, industrial designer, and engineer. He is the founder, CEO, and lead designer of SpaceX, Tesla\"\n",
        "\n",
        "inputs = tokenizer(question, text, return_tensors=\"pt\")\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Model phải đi tìm điểm bắt đầu để bắt đầu generate ra response\n",
        "# Điểm kểt thúc để xác định khi nào response sẽ kết thúc\n",
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()\n",
        "\n",
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "tokenizer.decode(predict_answer_tokens, skip_specials_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TiaRK-UU3Ee"
      },
      "outputs": [],
      "source": [
        "# Hallucination"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
