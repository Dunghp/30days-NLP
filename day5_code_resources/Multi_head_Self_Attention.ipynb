{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLlCaRtK8hFQ",
        "outputId": "1515b591-e0f0-4c22-bb95-fbcf68f6349d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Collecting torch\n",
            "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
            "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.5.1 (from torch)\n",
            "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/c2/f5/e1854cb2f2bcd4280c44736c93550cc300ff4b8c95ebe370d0aa7d2b473d/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvshmem-cu12\n",
            "    Found existing installation: nvidia-nvshmem-cu12 3.4.5\n",
            "    Uninstalling nvidia-nvshmem-cu12-3.4.5:\n",
            "      Successfully uninstalled nvidia-nvshmem-cu12-3.4.5\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.8.0+cu126\n",
            "    Uninstalling torch-2.8.0+cu126:\n",
            "      Successfully uninstalled torch-2.8.0+cu126\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.8.0+cu126 requires torch==2.8.0, but you have torch 2.9.1 which is incompatible.\n",
            "torchvision 0.23.0+cu126 requires torch==2.8.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 torch-2.9.1 triton-3.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torch --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, random, re, time\n",
        "\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "3enef26J8u3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code hàm set seed\n",
        "# Mục đích: đảm bảo được tính nhất quán trong quá trình training (ví dụ: đảm bảo các lần xáo dữ liệu đề như nhau)\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "8ficrcPK9ZoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm tokenize\n",
        "def simple_tokenize(text):\n",
        "    text = text.lower()\n",
        "    # Xử lý loại bỏ các ký tự đặc biệt như dấu câu, khoảng trắng dư thừa\n",
        "    tokens = re.findall(r\"[a-z0-9]+\", text)\n",
        "    # Nếu tokens rỗng => trả về [\"<empty>\"]\n",
        "    # Ngược lại, trả về tokens\n",
        "    return tokens if tokens else [\"<empty>\"]"
      ],
      "metadata": {
        "id": "z-SoZNda-Seb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "    def __init__(self, min_freq=2, max_size=50000, specials=None):\n",
        "        # Tạo một dictionary để đếm tần suất xuất hiện của token trong câu (key: token str, value: freq int)\n",
        "        self.freqs = {}\n",
        "        # Tạo một list để map ID về token (dùng để decode)\n",
        "        self.itos = []\n",
        "        # Tạo một dictionary để map token với ID (dùng để encode)\n",
        "        self.stoi = {}\n",
        "        # Giảm lượng vocab bằng cách loại bỏ các token xuất hiện quá ít trong câu\n",
        "        # Các token hiếm có thể gây ra noise trong quá trình huấn luyện\n",
        "        # Tốn bộ nhớ khi được đem đi tính \"attention\"\n",
        "        self.min_freq = min_freq\n",
        "        self.max_size = max_size\n",
        "        # Giới hạn kích thước vocab tối đa\n",
        "        # Tạo list các token đặc biệt\n",
        "        # <pad>: thêm vào tăng độ dài câu văn\n",
        "        # <unk>: các token lạ, không có trong bộ vocab\n",
        "        # <cls>: đánh dấu câu văn được dùng cho bài toán phân loại\n",
        "        self.specials = [\"<pad>\", \"<unk>\", \"<cls>\"]\n",
        "\n",
        "    # Đếm tần suất xuất hiện của các token trong câu\n",
        "    def add_token(self, token):\n",
        "        self.freqs[token] = self.freqs.get(token, 0) + 1\n",
        "\n",
        "    # Xây dựng bộ vocab\n",
        "    # Sau khi add_token từ bộ dataset, cần phải build để tạo stoi/itos\n",
        "    # Nếu mà không build => quá trình encode sẽ bị lỗi\n",
        "    def build(self):\n",
        "        self.itos = list(self.specials)\n",
        "        for i, sp in enumerate(self.specials):\n",
        "            self.stoi[sp] = i\n",
        "        # Sắp xếp tần suất xuất hiện tăng dần\n",
        "        # items = sorted([(t, f) for t, f in self.freqs.items() if t not in self.speicals])\n",
        "        # Sắp xếp tần suất xuất hiện giảm dần\n",
        "        items = sorted([(t, f) for t, f in self.freqs.items() if t not in self.specials],\n",
        "                       key=lambda x: (-x[1], x[0]))\n",
        "        for tok, freq in items:\n",
        "            if freq < self.min_freq or len(self.itos) >= self.max_size:\n",
        "                continue\n",
        "            self.stoi[tok] = len(self.itos)\n",
        "            self.itos.append(tok)\n",
        "\n",
        "    # Encode (tạo các input dạng số cho model)\n",
        "    def encode(self, tokens):\n",
        "        unk = self.stoi.get(\"<unk>\", 1)\n",
        "        return [self.stoi.get(token, unk) for token in tokens]\n",
        "\n",
        "    # Hai phương thức trả về ID của <pad> và <cls>\n",
        "    # static (Vocab.pad_idx())\n",
        "    # property (vocab = Vocab) => vocab.build() NO => YES vocab.pad_idx\n",
        "    @property\n",
        "    def pad_idx(self): return self.stoi[\"<pad>\"]\n",
        "    @property\n",
        "    def cls_idx(self): return self.stoi[\"<cls>\"]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)"
      ],
      "metadata": {
        "id": "qOk-POad_RXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextClsDataset(Dataset):\n",
        "    def __init__(self, texts, labels, vocab, max_len=64, add_cls=True):\n",
        "        # Tạo list lưu các văn bản\n",
        "        self.texts = texts\n",
        "        # Tạo list lưu các label của text\n",
        "        self.labels = labels\n",
        "        # Lưu bộ vocab\n",
        "        self.vocab = vocab\n",
        "        # Khai báo độ dài tối đa cho phép của một văn bản\n",
        "        self.max_len = max_len\n",
        "        self.add_cls = add_cls\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Lấy text và label tại idx, để xử lý từng sample cụ thể\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        # Text ban đầu: \"Tôi yêu pizza\"\n",
        "        # Tokenize text\n",
        "        tokens = simple_tokenize(text)\n",
        "        # => [\"tôi\", \"yêu\", \"pizza\"]\n",
        "        # Thêm token <cls> vào đầu văn bản, vì đây là bài toán phân loại text\n",
        "        if self.add_cls: tokens = [\"<cls>\"] + tokens\n",
        "        # => [\"<cls>\", \"tôi\", \"yêu\", \"pizza\"]\n",
        "\n",
        "        # Encode token thành ID\n",
        "        ids = self.vocab.encode(tokens)[:self.max_len] # Giới hạn độ dài văn bản\n",
        "        # Tạo một list attention mask\n",
        "        # Ý nghĩa: sẽ ra hiệu cho model biết được là token nào sẽ được áp phép \"attention\"\n",
        "        # Với <pad> => mô hình không được phép áp phép \"attention\"\n",
        "        attn = [1] * len(ids)\n",
        "        # Output: attn sẽ là [1, 1, ..., 1] cho token + [0, 0, ..., 0] cho các token <pad>\n",
        "        while len(ids) < self.max_len:\n",
        "            ids.append(self.vocab.pad_idx)\n",
        "            attn.append(0)\n",
        "\n",
        "        return torch.tensor(ids), torch.tensor(attn), torch.tensor(label)"
      ],
      "metadata": {
        "id": "ga7Hx9WMH_-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm collate giúp chúng ta tự động gom nhiều mẫu dữ liệu lại thành một batch để đưa vào train mô hình một lần\n",
        "def collate_fn(batch):\n",
        "    ids = torch.stack([b[0] for b in batch])\n",
        "    attn = torch.stack([b[1] for b in batch])\n",
        "    labels = torch.stack([b[2] for b in batch])\n",
        "    return ids, attn, labels"
      ],
      "metadata": {
        "id": "p046K19pModk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thiết kế kiến trúc Multi-Head Self-Attention - thành phần cốt lõi của Transformer Encoder\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Lưu tổng chiều embedding (D)\n",
        "        self.d_model = d_model\n",
        "        # Lưu số head (H), xử lý multi-head (song song)\n",
        "        self.num_heads = num_heads\n",
        "        # Tính chiều của mỗi head = (d_h = D / H)\n",
        "        self.d_head = d_model // num_heads\n",
        "\n",
        "        # Khai báo các Layer Linear cho Q, K, V\n",
        "        # Layer Linear sẽ được sử dụng để project input thành Query (Q), tương tự cho Key (K) và Value (V)\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        # Đây là các ma trận parameters sẽ được đưa vào huấn luyện => giúp model học cách biến đổi input\n",
        "\n",
        "        # Layer project output sau khi input đã được tổng hợp từ các head (d_head)\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout) # Giúp tránh overfit\n",
        "\n",
        "    def forward(self, x, attn_mask):\n",
        "        # Lấy ra shape của input\n",
        "        # B: Batch size\n",
        "        # L: Sequence Length (độ dài của chuỗi sau khi padding)\n",
        "        # D: Embedding Dimension (số chiều của vector biểu diễn mỗi token)\n",
        "        B, L, D = x.shape\n",
        "        H = self.num_heads\n",
        "        d_h = self.d_head\n",
        "\n",
        "        # 1. Project x thành Q qua W_q\n",
        "        # 2. Reshape từ B, L, D sang (B, L, H, d_h) chuyển thành (B, H, L, d_h) (chuẩn bị cho bước tính muti-head self-attention)\n",
        "        # => Chia D thành H head, mỗi head có d_h chiều\n",
        "        Q = self.W_q(x).view(B, L, H, d_h).transpose(1, 2)\n",
        "        # Tương tự cho K và V\n",
        "        K = self.W_k(x).view(B, L, H, d_h).transpose(1, 2)\n",
        "        V = self.W_v(x).view(B, L, H, d_h).transpose(1, 2)\n",
        "\n",
        "        # Tính attention score\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1))/math.sqrt(d_h)\n",
        "\n",
        "        mask = attn_mask.unsqueeze(1).unsqueeze(2)\n",
        "        scores = scores.masked_fill(mask==0, float(\"-inf\"))\n",
        "        attn = F.softmax(scores, dim=-1) # Biến scores thành xác suất\n",
        "        attn = self.dropout(attn) # Tránh overfit\n",
        "\n",
        "        # Tính output: attn @ V - nhân weights với value\n",
        "        out = torch.matmul(attn, V).transpose(1, 2).contiguous().view(B, L, D)\n",
        "        # Project output cuối qua W_o để tổng hợp thông tin từ multi-head\n",
        "        out = self.W_o(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "6-z-atxCNJ6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Triển khai kiến trúc Transformer Encoder\n",
        "# Self-Attention + AddNorm + Feed Forward Neural Network + AddNorm\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Khởi tạo lớp Multi-Head Self-Attention\n",
        "        self.mha = MultiHeadSelfAttention(d_model, num_heads, dropout)\n",
        "        # Ổn định quá trình huấn luyện (tránh gradient vanish/explode)\n",
        "        # Giúp cho mô hình hội tụ nhanh hơn => Học nhanh hơn\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        # Feed Forward Neural Network\n",
        "        # Thêm tính phi tuyến => giúp model học được các biểu diễn phức tạp sau khi các token đã được áp phép \"attention\"\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,x,attn_mask):\n",
        "        # tính Self-Attention: self.mha(x, attn_mask) – gọi lớp MHA để tạo representation mới (dùng Q, K, V từ x, áp dụng mask).\n",
        "        x = self.norm1(x + self.dropout(self.mha(x,attn_mask)))\n",
        "        # Tính FFN: self.ff(x) – project lên d_ff, ReLU, drop, project về d_model.\n",
        "        x = self.norm2(x + self.dropout(self.ff(x)))\n",
        "        return x"
      ],
      "metadata": {
        "id": "vS44wKd5S_4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        # Tạo một ma trận rỗng để lưu positional encoding cho mọi token từ vị trí 0 đến max_len - 1\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        # Tạo vector vị trí: 0, 1, 2, ..., 511 => mỗi hàng đại diện cho một vị trí của token trong câu\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        # Tạo một dãy số giảm dần theo cấp số nhân => Tạo tần số khác nhau cho mỗi chiều vector\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0)/d_model))\n",
        "        # Áp dụng sin/cos để đảm bảo mỗi vị trí là một vector độc nhất\n",
        "        # Chiều chẵn => dùng sin\n",
        "        pe[:,0::2] = torch.sin(position * div_term)\n",
        "        # Chiều lẻ => dùng cos\n",
        "        pe[:,1::2] = torch.cos(position * div_term)\n",
        "        # pe là không có train (không tính gradient) => giống hằng số đã tính sẵn\n",
        "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
        "\n",
        "    # x = embedding(x) + positional_encoding(position)\n",
        "    def forward(self,x):\n",
        "        L = x.size(1)\n",
        "        return x+self.pe[:,:L,:]"
      ],
      "metadata": {
        "id": "q18M1B1nV_Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier: Embedding => Positional Encoding => Một loạt các lớp TransformerEncoderLayer => AddNorm => Linear\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, num_classes, d_model=128, num_heads=4, num_layers=2, d_ff=256, max_len=128, pad_idx=0, dropout=0.1):\n",
        "        super().__init__()\n",
        "        # Input là sequence ID => chuyển thành vector trước khi thêm positional\n",
        "        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)\n",
        "        # Thêm positional => thêm \"vị trí\" cho token trước khi qua lớp Encoder\n",
        "        self.pos = PositionalEncoding(d_model, max_len=max_len)\n",
        "        # Khai báo hàng loạt các lớp Transformer Encoder\n",
        "        self.layers = nn.ModuleList(\n",
        "            [TransformerEncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.cls_head = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self,ids,attn_mask):\n",
        "        x = self.embed(ids)\n",
        "        x = self.pos(x)\n",
        "        for l in self.layers: x = l(x,attn_mask)\n",
        "        x = self.norm(x)\n",
        "        cls_rep = x[:,0,:]\n",
        "        return self.cls_head(cls_rep)"
      ],
      "metadata": {
        "id": "DtASvONYZPtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# 7. Training\n",
        "# ------------------------------\n",
        "def train_one_epoch(model,loader,opt,device,criterion):\n",
        "    model.train()\n",
        "    total_loss,total,correct=0,0,0\n",
        "    for ids,attn,labels in loader:\n",
        "        ids,attn,labels=ids.to(device),attn.to(device),labels.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits=model(ids,attn)\n",
        "        loss=criterion(logits,labels)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        total_loss+=loss.item()*ids.size(0)\n",
        "        total+=ids.size(0)\n",
        "        correct+=(logits.argmax(-1)==labels).sum().item()\n",
        "    return total_loss/total, correct/total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model,loader,device,criterion):\n",
        "    model.eval()\n",
        "    total_loss,total,correct=0,0,0\n",
        "    for ids,attn,labels in loader:\n",
        "        ids,attn,labels=ids.to(device),attn.to(device),labels.to(device)\n",
        "        logits=model(ids,attn)\n",
        "        loss=criterion(logits,labels)\n",
        "        total_loss+=loss.item()*ids.size(0)\n",
        "        total+=ids.size(0)\n",
        "        correct+=(logits.argmax(-1)==labels).sum().item()\n",
        "    return total_loss/total, correct/total"
      ],
      "metadata": {
        "id": "Vl6xGX3_bncr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main**"
      ],
      "metadata": {
        "id": "s1pM6siBb0li"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca_E7u_vbz4x",
        "outputId": "259f5ea7-0bc3-4c3d-af54-10899a99f6f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kagglehub) (25.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from kagglehub) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kagglehub) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->kagglehub) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Tải dataset\n",
        "path = kagglehub.dataset_download(\"amananandrai/ag-news-classification-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAj-1t4nb54Z",
        "outputId": "707a0ebd-1024-4e3a-dee6-073f27ab4b06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'ag-news-classification-dataset' dataset.\n",
            "Path to dataset files: /kaggle/input/ag-news-classification-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(os.path.join(path, \"train.csv\"), header=0, names=[\"class\", \"title\", \"desc\"])\n",
        "test_df = pd.read_csv(os.path.join(path, \"test.csv\"), header=0, names=[\"class\", \"title\", \"desc\"])"
      ],
      "metadata": {
        "id": "jhhZGbSycCJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tiền xử lý dữ liệu\n",
        "def preprocess_agnews(df):\n",
        "    # Ép kiểu int, trừ 1 để zero-based\n",
        "    labels = df[\"class\"].astype(int) - 1\n",
        "    texts = (df[\"title\"].astype(str) + \" \" + df[\"desc\"].astype(str)).tolist()\n",
        "    return texts, labels.tolist()"
      ],
      "metadata": {
        "id": "IBGej_Wqci2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, y_train = preprocess_agnews(train_df)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, shuffle=True)\n",
        "X_test, y_test = preprocess_agnews(test_df)"
      ],
      "metadata": {
        "id": "bO6yGc6HcsWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Xây vocab từ dữ liệu train\n",
        "vocab = Vocab(min_freq=5)\n",
        "for sentences in X_train:\n",
        "    for token in simple_tokenize(sentences):\n",
        "        vocab.add_token(token)\n",
        "vocab.build()"
      ],
      "metadata": {
        "id": "_CkUItktc0ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tạo các Dataloader\n",
        "max_len = 64\n",
        "batch_size=128\n",
        "\n",
        "train_dataset = TextClsDataset(X_train, y_train, vocab, max_len)\n",
        "val_dataset = TextClsDataset(X_val, y_train, vocab, max_len)\n",
        "test_dataset = TextClsDataset(X_test, y_test, vocab, max_len)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "num_classes = 4"
      ],
      "metadata": {
        "id": "xq_qx0WjdZgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "HdttHs0wev7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo mô hình\n",
        "transformer = TransformerClassifier(len(vocab), num_classes).to(device)\n",
        "optimizer = torch.optim.AdamW(transformer.parameters(), lr=2e-3)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "O4HIXJnVe2gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(3):\n",
        "    train_loss, train_accuracy = train_one_epoch(transformer, train_loader, optimizer, device, criterion)\n",
        "    val_loss, val_accuracy = evaluate(transformer, val_loader, device, criterion)\n",
        "    print(f\"Epoch {epoch}: train_acc={train_accuracy:.2f} val_acc={val_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Sw2plZOgRs0",
        "outputId": "3ce4781a-34be-4965-ce79-3c70422c39ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: train_acc=0.82 val_acc=0.25\n",
            "Epoch 1: train_acc=0.90 val_acc=0.25\n",
            "Epoch 2: train_acc=0.92 val_acc=0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = evaluate(transformer, test_loader, device, criterion)\n",
        "print(f\"test_acc={train_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpkVndXNjol7",
        "outputId": "f26e7333-a005-4b1e-bcdb-f19e20ba376e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_acc=0.92\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# 11. Inference / Testing\n",
        "# ------------------------------\n",
        "\n",
        "def predict(model, text, vocab, max_len=64, add_cls=True):\n",
        "    model.eval()\n",
        "    toks = simple_tokenize(text)\n",
        "    if add_cls:\n",
        "        toks = [\"<cls>\"] + toks\n",
        "    ids = vocab.encode(toks)[:max_len]\n",
        "    attn = [1] * len(ids)\n",
        "    while len(ids) < max_len:\n",
        "        ids.append(vocab.pad_idx)\n",
        "        attn.append(0)\n",
        "\n",
        "    ids = torch.tensor(ids).unsqueeze(0).to(device)\n",
        "    attn = torch.tensor(attn).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(ids, attn)\n",
        "        pred = logits.argmax(-1).item()\n",
        "    return pred\n",
        "\n",
        "# Label mapping cho AG News\n",
        "id2label = {\n",
        "    0: \"World\",\n",
        "    1: \"Sports\",\n",
        "    2: \"Business\",\n",
        "    3: \"Sci/Tech\"\n",
        "}\n",
        "\n",
        "# Một vài câu test\n",
        "test_sentences = [\n",
        "    \"NASA launches new satellite to explore Mars surface\",\n",
        "    \"The stock market crashed due to inflation fears\",\n",
        "    \"Manchester United won the Premier League title\",\n",
        "    \"President meets with world leaders to discuss climate change\"\n",
        "]\n",
        "\n",
        "print(\"=== Transformer Predictions ===\")\n",
        "for sent in test_sentences:\n",
        "    pred = predict(transformer, sent, vocab)\n",
        "    print(f\"Text: {sent}\\n -> Predicted: {id2label[pred]}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj7E44EnkLLS",
        "outputId": "84b95e98-32c3-46e7-e185-383719172dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Transformer Predictions ===\n",
            "Text: NASA launches new satellite to explore Mars surface\n",
            " -> Predicted: Sci/Tech\n",
            "\n",
            "Text: The stock market crashed due to inflation fears\n",
            " -> Predicted: Business\n",
            "\n",
            "Text: Manchester United won the Premier League title\n",
            " -> Predicted: Sports\n",
            "\n",
            "Text: President meets with world leaders to discuss climate change\n",
            " -> Predicted: World\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8V5Lvtk6kgva"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}