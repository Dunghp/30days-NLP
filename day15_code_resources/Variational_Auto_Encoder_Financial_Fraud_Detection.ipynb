{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik-2FB0cXFzF",
        "outputId": "95291e20-0381-40aa-f11f-c791e650c678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Cho model học từ giao dịch bình thường => khi test fraud, nếu reconstruction error cao (dữ liệu được tạo ra bị lệch khỏi phân phối Gaussian quá nhiều) => \"Bất thường\"\n",
        "\n",
        "+ Nếu các bạn dùng AE, thì fraud vẫn có thể được tại tạo tốt"
      ],
      "metadata": {
        "id": "I0c-pc3GYEjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "2GBbZ4R2XdkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Đọc data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ML_DL_datasets/Synthetic_Financial_datasets_log.csv\")\n",
        "df = df.drop(['nameOrig', 'nameDest', 'isFlaggedFraud'], axis=1)"
      ],
      "metadata": {
        "id": "i6FdnS3vZFC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Phân loại normal và fraud\n",
        "normal_df = df[df[\"isFraud\"] == 0]\n",
        "fraud_df = df[df[\"isFraud\"] == 1]"
      ],
      "metadata": {
        "id": "tkK7mcHkZYgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Features\n",
        "categorical_features = ['type']\n",
        "numeric_features = ['step', 'amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']"
      ],
      "metadata": {
        "id": "10ds8F06ZyFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thiết lập pipeline preprocess data trước khi đưa vào model (xử lý cho normal_df)\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numeric_features),\n",
        "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "0GL9wxSyZ-5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit và transform normal_df\n",
        "X_normal = preprocessor.fit_transform(normal_df.drop('isFraud', axis=1))"
      ],
      "metadata": {
        "id": "5THdFG2vaiWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform toàn bộ để test\n",
        "X_all = preprocessor.transform(df.drop('isFraud', axis=1))\n",
        "labels = df[\"isFraud\"].values"
      ],
      "metadata": {
        "id": "GtUEVEXea5C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to tensor\n",
        "normal_tensor = torch.tensor(X_normal, dtype=torch.float32)\n",
        "all_tensor = torch.tensor(X_all, dtype=torch.float32)\n",
        "\n",
        "# Dataloader cho normal\n",
        "dataset = TensorDataset(normal_tensor, normal_tensor)\n",
        "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)"
      ],
      "metadata": {
        "id": "K2fhGU2dbJvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = torch.relu(self.fc1(x))\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        return mu, logvar\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.fc1 = nn.Linear(latent_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, z):\n",
        "        h = torch.relu(self.fc1(z))\n",
        "        x_hat = torch.sigmoid(self.fc2(h))  # Nếu data không [0,1], thay bằng: x_hat = self.fc2(h)\n",
        "        return x_hat"
      ],
      "metadata": {
        "id": "0hkBC5jZbXqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "        super(VAE, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, hidden_dim, latent_dim)\n",
        "        self.decoder = Decoder(latent_dim, hidden_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Tính được mean của phân phối Gaussian (mu)\n",
        "        mu, logvar = self.encoder(x)\n",
        "        # Variance: phương sai (logvar * 0.5)\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        # eps là noise (đáp ứng được yêu cầu N(0,1))\n",
        "        # N(0, 1) là ký hiệu toán học để chỉ phân phối Gaussian chuẩn (mean=0, std=1)\n",
        "        z = mu + std * eps # latent vector\n",
        "        x_hat = self.decoder(z)\n",
        "        return x_hat, mu, logvar"
      ],
      "metadata": {
        "id": "D_UYub7LbxeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function\n",
        "def vae_loss(recon_x, x, mu, logvar):\n",
        "    # đo lường sự khác biệt giữa input gốc (x) và output tạo mới (recon_x)\n",
        "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction=\"sum\")\n",
        "    # KL loss (Kullback-Leibler divergence) như kiểm tra \"bạn có khớp với đám đông không?\"\n",
        "    # N(0, 1) => tiêu chuẩn của đám đông => Kiểm tra z từ data có khớp với tiêu chuẩn không?\n",
        "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + kl_loss # tính tổng loss để optimizer minimize"
      ],
      "metadata": {
        "id": "KDka8bCYddiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = X_normal.shape[1]\n",
        "hidden_dim = 64\n",
        "latent_dim = 4\n",
        "epochs = 50"
      ],
      "metadata": {
        "id": "qy_OY6WPevYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VAE(input_dim, hidden_dim, latent_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "vXqnp5o3fFLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        inputs, _ = data\n",
        "        recon, mu, logvar = model(inputs)\n",
        "        loss = vae_loss(recon, inputs, mu, logvar)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(dataloader.dataset)\n",
        "    print(f'Epoch {epoch+1}, Avg Loss: {avg_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WuNdWYU4fOuY",
        "outputId": "36f344c5-08c9-427f-fa4a-27e06ba52f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Avg Loss: 5.9940\n",
            "Epoch 2, Avg Loss: 5.9475\n",
            "Epoch 3, Avg Loss: 5.9429\n",
            "Epoch 4, Avg Loss: 5.9375\n",
            "Epoch 5, Avg Loss: 5.9335\n",
            "Epoch 6, Avg Loss: 5.9365\n",
            "Epoch 7, Avg Loss: 5.9309\n",
            "Epoch 8, Avg Loss: 5.9300\n",
            "Epoch 9, Avg Loss: 5.9291\n",
            "Epoch 10, Avg Loss: 5.9279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tính anomaly score\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    recon_all, mu_all, logvar_all = model(all_tensor)\n",
        "    recon_errors = torch.mean((recon_all - all_tensor)**2, dim=1)\n",
        "    kl_divs = -0.5 * torch.sum(1 + logvar_all - mu_all.pow(2) - logvar_all.exp(), dim=1)\n",
        "    scores = recon_errors + kl_divs  # Kết hợp\n",
        "    scores = scores.numpy()"
      ],
      "metadata": {
        "id": "daIIXgw_fcWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normal_scores = scores[:len(normal_df)]\n",
        "threshold = np.mean(normal_scores) + 3 * np.std(normal_scores)\n",
        "print(f'Threshold: {threshold:.4f}')"
      ],
      "metadata": {
        "id": "wvgswnRvfd_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "auc = roc_auc_score(labels, scores)\n",
        "print(f'AUC Score: {auc:.4f}')"
      ],
      "metadata": {
        "id": "XZKUCQ3rfsmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}