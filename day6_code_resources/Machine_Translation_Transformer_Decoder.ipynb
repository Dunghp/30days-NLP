{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOtAxw5xZcTr",
        "outputId": "9d458969-a569-4f87-b7d9-b2ad3e895fe5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.16.2\n",
            "  Downloading torchtext-0.16.2-cp312-cp312-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext==0.16.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext==0.16.2) (2.32.4)\n",
            "Collecting torch==2.2.0 (from torchtext==0.16.2)\n",
            "  Downloading torch-2.2.0-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext==0.16.2) (2.0.2)\n",
            "Collecting torchdata==0.7.1 (from torchtext==0.16.2)\n",
            "  Downloading torchdata-0.7.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->torchtext==0.16.2) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0->torchtext==0.16.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0->torchtext==0.16.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0->torchtext==0.16.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0->torchtext==0.16.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0->torchtext==0.16.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0->torchtext==0.16.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0->torchtext==0.16.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0->torchtext==0.16.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0->torchtext==0.16.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0->torchtext==0.16.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0->torchtext==0.16.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.0->torchtext==0.16.2)\n",
            "  Downloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.12/dist-packages (from torchdata==0.7.1->torchtext==0.16.2) (2.5.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->torchtext==0.16.2) (12.6.85)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.16.2) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.16.2) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.16.2) (2025.11.12)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.0->torchtext==0.16.2) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.0->torchtext==0.16.2) (1.3.0)\n",
            "Downloading torchtext-0.16.2-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.0-cp312-cp312-manylinux1_x86_64.whl (755.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.4/755.4 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.7.1-py3-none-any.whl (184 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.4/184.4 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchdata, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu126\n",
            "    Uninstalling torch-2.9.0+cu126:\n",
            "      Successfully uninstalled torch-2.9.0+cu126\n",
            "  Attempting uninstall: torchdata\n",
            "    Found existing installation: torchdata 0.11.0\n",
            "    Uninstalling torchdata-0.11.0:\n",
            "      Successfully uninstalled torchdata-0.11.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.2.0 which is incompatible.\n",
            "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.1 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.0 torchdata-0.7.1 torchtext-0.16.2 triton-2.2.0\n",
            "Collecting portalocker==2.5.1\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.16.2\n",
        "!pip install portalocker==2.5.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import math\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "vB6vd83bal_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tạo class quản lý dataset**"
      ],
      "metadata": {
        "id": "UjL1d_fDdteY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tạo class quản lý dataset\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, csv_file, src_lang, tgt_lang):\n",
        "        # Khởi tạo một list rỗng lưu các cặp src, tgt\n",
        "        self.data = []\n",
        "        df = pd.read_csv(csv_file)[:20000] # Lấy 20_000 samples\n",
        "        for _, row in df.iterrows():\n",
        "            # Lấy các câu từ src_lang và tgt_lang\n",
        "            src, tgt = row[src_lang], row[tgt_lang]\n",
        "            # Thêm cặp src, tgt dưới dạng tuple\n",
        "            self.data.append((src, tgt))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.data[idx]\n",
        "        return src, tgt"
      ],
      "metadata": {
        "id": "0HDl2AVvcLD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo tokenize (chung)\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "def tokenize(text):\n",
        "    return tokenizer(text)"
      ],
      "metadata": {
        "id": "DFiCltd8dP_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tạo dataset và bộ từ điển riêng cho**"
      ],
      "metadata": {
        "id": "Th7yxuw5dqQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo dataset\n",
        "dataset = TranslationDataset(\"/content/drive/MyDrive/ML_DL_datasets/en_fr_dataset.csv\", src_lang=\"English words/sentences\", tgt_lang=\"French words/sentences\")"
      ],
      "metadata": {
        "id": "INCOz7TKdhQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tạo hai bộ từ điển (1 cho src, 1 cho tgt)\n",
        "tokenizer_src = get_tokenizer('basic_english')\n",
        "tokenizer_tgt = get_tokenizer('basic_english')"
      ],
      "metadata": {
        "id": "erzeKge3eGJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yield_tokens(data_iter, lang):\n",
        "    for src, tgt in data_iter:\n",
        "        if lang == 'src':\n",
        "            yield tokenizer_src(src)\n",
        "        else:\n",
        "            yield tokenizer_tgt(tgt)"
      ],
      "metadata": {
        "id": "GXIWJrFJfmH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab = build_vocab_from_iterator(yield_tokens(dataset, 'src'), specials=['<pad>', '<sos>', '<eos>', '<unk>'])\n",
        "tgt_vocab = build_vocab_from_iterator(yield_tokens(dataset, 'tgt'), specials=['<pad>', '<sos>', '<eos>', '<unk>'])\n",
        "src_vocab.set_default_index(src_vocab['<unk>'])\n",
        "tgt_vocab.set_default_index(tgt_vocab['<unk>'])"
      ],
      "metadata": {
        "id": "FdOyZq72f9LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab['<sos>']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cf2hv2xg6hQ",
        "outputId": "4e22d90e-5035-49ea-c90e-44f9effdeba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab['<eos>']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfR0pG2pg-JY",
        "outputId": "f35516e9-ee4b-4366-d5a3-2f60a9092751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab['<pad>']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ik2h-WqghA4K",
        "outputId": "6fd16141-c2a0-4a85-ef9e-92684bca8a06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab['<unk>']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_XKS0K7hCmQ",
        "outputId": "0687e5d1-d716-42f2-bb07-3218592c3aca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm collate function\n",
        "# Giúp Pytorch xử lý và biến đổi data đầu vào thành các batch => định dạng phù hợp để huấn luyện mô hình\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src, tgt in batch:\n",
        "        # Token hóa src (dùng tokenizer)\n",
        "        # Map từng token thành index từ src_vocab\n",
        "        # Thêm '<sos>' và '<eos>'\n",
        "        src_batch.append([src_vocab['<sos>']] + [src_vocab[t] for t in tokenizer_src(src)] + [src_vocab['<eos>']])\n",
        "        tgt_batch.append([tgt_vocab['<sos>']] + [tgt_vocab[t] for t in tokenizer_tgt(tgt)] + [tgt_vocab['<eos>']])\n",
        "\n",
        "    # pad các tensor để đảm bảo các câu có độ dài bằng nhau\n",
        "    src_batch = torch.nn.utils.rnn.pad_sequence([torch.tensor(s) for s in src_batch], padding_value=src_vocab['<pad>'])\n",
        "    tgt_batch = torch.nn.utils.rnn.pad_sequence([torch.tensor(t) for t in tgt_batch], padding_value=tgt_vocab['<pad>'])\n",
        "\n",
        "    return src_batch.transpose(0, 1).contiguous(), tgt_batch.transpose(0, 1).contiguous()"
      ],
      "metadata": {
        "id": "0fNViNm8hDyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tạo Dataloader\n",
        "train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "jYuEIol5jr88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build model**"
      ],
      "metadata": {
        "id": "OIvqLsOYkxRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x"
      ],
      "metadata": {
        "id": "2MFbXY4qj7J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Multi-Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "        # Đây là các ma trận parameters sẽ được đưa vào huấn luyện => giúp model học cách biến đổi input\n",
        "\n",
        "        # Layer project output sau khi input đã được tổng hợp từ các head (d_head)\n",
        "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        output = torch.matmul(attn, V)\n",
        "        return output, attn\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "\n",
        "        Q = self.W_q(Q).reshape(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(K).reshape(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(V).reshape(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        # Attention\n",
        "        output, attn = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        output = self.W_o(output)\n",
        "        return output, attn"
      ],
      "metadata": {
        "id": "9xd_3QssmNhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Feed Forward Neural Netwwork\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "9_bg7hcCo06q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Encoder\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-Attention\n",
        "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        # Feed Forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "_FNUFODOpkU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Decoder Layer\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        # Có 2 attention\n",
        "        # 1. self_attn giúp model học được mối liên hệ giữa các token với nhau trong câu target\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        # 2. cross_attn giúp cho model tham chiếu tới phần src => cross_attn liên kết src với tgt\n",
        "        # Không có cross_attn => decoder không biết dịch vì không có tham chiếu đến src\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        attn_output, _ = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        # Cross attention => sinh ra target sequence kết hợp tham chiếu đến src thông qua cross_attn\n",
        "        attn_output, _ = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        # Feed Forward\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "7qKfK6tuqquT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Transformer Model\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, num_layers=6, d_ff=2048, max_seq_len=100, dropout=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.decoder_layers = nn.ModuleList(\n",
        "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    # generate_mask\n",
        "    # che đi các vị trí <pad> => những vị trí không có ý nghĩa\n",
        "    # Che đi các từ phía sau, chỉ cho phép model dự đoán từ tiếp theo dựa trên các từ phía trước\n",
        "    # => Quan trọng để model không \"gian lận\" trong quá trình training\n",
        "    def generate_mask(self, src, tgt):\n",
        "        # src != 0 (không phải <pad>) => lấy\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "        seq_length = tgt.size(1)\n",
        "\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
        "        # Kết hợp padding mask với nopeak mask\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "zyu3RklLr_5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Model**"
      ],
      "metadata": {
        "id": "iGmkd-cxz-iO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Khởi tạo mô hình\n",
        "model = Transformer(len(src_vocab), len(tgt_vocab), d_model=512, num_heads=8, num_layers=6, d_ff=2048)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=src_vocab['<pad>'])"
      ],
      "metadata": {
        "id": "9YjcYLuqz7Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Huấn luyện với tqdm\n",
        "model.train()\n",
        "for epoch in range(10):\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    # Thêm tqdm cho vòng lặp batch\n",
        "    progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch}', leave=True)\n",
        "    for src, tgt in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt[:, :-1])\n",
        "        loss = criterion(output.reshape(-1, len(tgt_vocab)), tgt[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "        # Cập nhật thanh tiến độ với loss hiện tại\n",
        "        progress_bar.set_postfix({'batch_loss': loss.item()})\n",
        "    avg_loss = total_loss / num_batches\n",
        "    print(f'Epoch {epoch}, Average Loss: {avg_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYm-wP4E0o-U",
        "outputId": "f814c7ef-f677-4c11-c755-5cfbbc023638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 625/625 [03:51<00:00,  2.70it/s, batch_loss=2.77]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Average Loss: 3.0647\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 625/625 [03:52<00:00,  2.69it/s, batch_loss=1.77]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Average Loss: 1.9192\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 625/625 [03:52<00:00,  2.69it/s, batch_loss=1.04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Average Loss: 1.4313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 625/625 [03:52<00:00,  2.68it/s, batch_loss=1.19]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Average Loss: 1.1028\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 625/625 [03:52<00:00,  2.69it/s, batch_loss=0.619]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Average Loss: 0.8691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 625/625 [03:55<00:00,  2.65it/s, batch_loss=0.755]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Average Loss: 0.6930\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 625/625 [03:53<00:00,  2.68it/s, batch_loss=0.665]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Average Loss: 0.5658\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 625/625 [03:53<00:00,  2.68it/s, batch_loss=0.542]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Average Loss: 0.4719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8:  52%|█████▏    | 325/625 [02:01<01:46,  2.82it/s, batch_loss=0.427]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, model, src_vocab, tgt_vocab, max_length=20):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Token hóa câu input\n",
        "        tokens = [src_vocab['<sos>']]\n",
        "        for t in tokenizer(sentence):\n",
        "            tokens.append(src_vocab[t] if t in src_vocab.get_itos() else src_vocab['<unk>'])\n",
        "        tokens.append(src_vocab['<eos>'])\n",
        "        src_tensor = torch.LongTensor(tokens).unsqueeze(0)\n",
        "\n",
        "        # Khởi tạo tensor đầu ra với <sos>\n",
        "        # Bắt đầu sinh câu dịch bằng <sos> — giống như decoder trong training\n",
        "        tgt_tokens = [tgt_vocab['<sos>']]\n",
        "        # Tối đa sinh max_length từ (20) → tránh vòng lặp vô hạn\n",
        "        for _ in range(max_length):\n",
        "            # Chuyển list token đã sinh → tensor, thêm batch dim → (1, current_len)\n",
        "            tgt_tensor = torch.LongTensor(tgt_tokens).unsqueeze(0)\n",
        "            # Use the full tgt_tensor for the first prediction\n",
        "            # → Lần đầu: chỉ có <sos> → đưa nguyên tgt_tensor\n",
        "            # → Từ lần thứ 2: đưa tgt_tensor[:, :-1] (loại bỏ token cuối cùng vừa sinh)\n",
        "            # → vì model được train với teacher forcing (input là ground truth trừ token cuối)\n",
        "            output = model(src_tensor, tgt_tensor if len(tgt_tokens) == 1 else tgt_tensor[:, :-1])\n",
        "            # output: shape (1, current_len, vocab_size)\n",
        "            # argmax(dim=-1): lấy token có xác suất cao nhất mỗi vị trí\n",
        "            # [:, -1]: chỉ lấy dự đoán ở vị trí cuối cùng (vừa sinh)\n",
        "            # .item(): chuyển tensor 1 phần tử → số nguyên Python\n",
        "            pred_token = output.argmax(dim=-1)[:, -1].item()\n",
        "            # Thêm token vừa dự đoán vào chuỗi đang sinh\n",
        "            tgt_tokens.append(pred_token)\n",
        "            # Dừng khi pred_token là <eos> => kết thúc\n",
        "            if pred_token == tgt_vocab['<eos>']:\n",
        "                break\n",
        "\n",
        "        # Giải mã thành văn bản\n",
        "        # Bỏ <sos> ở đầu\n",
        "        # Loại bỏ <pad> và <eos>\n",
        "        # → danh sách từ theo index → lấy từ tương ứng\n",
        "        # Ghép các từ bằng dấu cách\n",
        "        translation = [tgt_vocab.get_itos()[i] for i in tgt_tokens[1:] if i not in [tgt_vocab['<pad>'], tgt_vocab['<eos>']]]\n",
        "        return ' '.join(translation)"
      ],
      "metadata": {
        "id": "2bk2QnfG0vYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test với input tùy chỉnh\n",
        "test_sentences = [\n",
        "    \"Hello world\",\n",
        "    \"How are you?\",\n",
        "    \"My name is Marie\",\n",
        "    \"What time is it?\"\n",
        "]"
      ],
      "metadata": {
        "id": "5fsboiho5984"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in test_sentences:\n",
        "    translated = translate_sentence(sentence, model, src_vocab, tgt_vocab)\n",
        "    print(f\"English: {sentence}\")\n",
        "    print(f\"French: {translated}\")\n",
        "    print(\"---\")"
      ],
      "metadata": {
        "id": "WuK6J-f96FL2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}